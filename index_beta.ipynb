{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.context.llm_context import LLMContext\n",
    "from llm.strategy.haiku3_model import Haiku3Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"system\"] = \"You are a helpful AI assistant for travel tips and recommendations\"\n",
    "data[\"user\"] = \"Country of Paris?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = LLMContext(Haiku3Model())\n",
    "text_generated = context.generate_response(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris is the capital city of France. France is the country that Paris is located in.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcolorama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init, Fore\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/errors.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/compat.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/thinc/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/thinc/config.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/thinc/types.py:25\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     Any,\n\u001b[1;32m      6\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     overload,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy, has_cupy\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[1;32m     28\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/thinc/compat.py:99\u001b[0m\n\u001b[1;32m     95\u001b[0m has_mxnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     h5py \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/h5py/__init__.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mhdf5_version_tuple \u001b[38;5;241m!=\u001b[39m version\u001b[38;5;241m.\u001b[39mhdf5_built_version_tuple:\n\u001b[1;32m     36\u001b[0m     _warn((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5py is running against HDF5 \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m when it was built against \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis may cause problems\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_version_tuple),\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_built_version_tuple)\n\u001b[1;32m     40\u001b[0m     ))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/h5py/version.py:15\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Versioning module for h5py.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m h5 \u001b[38;5;28;01mas\u001b[39;00m _h5\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n",
      "File \u001b[0;32mh5py/h5.pyx:15\u001b[0m, in \u001b[0;36minit h5py.h5\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:398\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import threading\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import spacy\n",
    "import numpy as np \n",
    "from colorama import init, Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(autoreset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"meta.llama3-70b-instruct-v1:0\", #Big Llama\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\", #Big Claude\n",
    "    \"anthropic.claude-3-5-sonnet-20240620-v1:0\", #Last Claude\n",
    "    \"anthropic.claude-3-sonnet-20240229-v1:0\", #Last middle Claude\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\", #Big Mistral\n",
    "    \"meta.llama3-8b-instruct-v1:0\" #Small Llama\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Devuelve solo el texto original con la identificación y etiquetación del texto con la información personal del paciente añadiendolo entre claudators, por ejemplo: [**arquitecto**], [**8/9/21**], [**padre**]. No comentes nada más\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poner -> Etiquetas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an anonymization tool in identifying attributes in texts that can identify or quasi-identify a user.\n",
    "Return only the original text with the identification and labeling of the patient's personal information by adding it between [** and **].\n",
    "Following are attributes that you must anonymize.\n",
    "\n",
    "- Names\n",
    "Example:\n",
    "En seguimiento por Hematología Centro Médico Aspasia (Dra. Valvanera).   ->\n",
    "En seguimiento por Hematología [**Centro Médico Aspasia**] (Dra. [**Valvanera**]).\n",
    "\n",
    "- Ages\n",
    "Example: \n",
    "Varón de 41 años.   ->\n",
    "[**Varón**] de [**41 años**].\n",
    "\n",
    "- Sexes\n",
    "Example: \n",
    "Varón de 41 años.   ->\n",
    "[**Varón**] de [**41 años**].\n",
    "\n",
    "- Professions\n",
    "Example: \n",
    "Trabaja como profesor.   ->\n",
    "Trabaja como [**profesor**].\n",
    "\n",
    "- Relatives\n",
    "Example: \n",
    "Vive con suegro y 2 yernos.   ->\n",
    "Vive con [**suegro**] y 2 [**yernos**].\n",
    "\n",
    "- Dates\n",
    "Example: \n",
    "ha estado viviendo en el Centro desde septiembre de 2008.   ->\n",
    "ha estado viviendo en el [**Centro**] desde [**septiembre de 2008**].\n",
    "\n",
    "- Phone numbers\n",
    "Example: \n",
    "contactando con el siguiente número de teléfono +50 88 078 68 49.   ->\n",
    "contactando con el siguiente número de teléfono [**+50 88 078 68 49**].\n",
    "\n",
    "- Identification numbers\n",
    "Example:\n",
    "El paciente otorga su consentimiento informado para participar en el estudio del protocolo WYX/8408/5545.   ->\n",
    "El paciente otorga su consentimiento informado para participar en el estudio del protocolo [**WYX/8408/5545.**]\n",
    "\n",
    "- Institutions, hospitals, health centers, etc\n",
    "Example: \n",
    "En seguimiento por Hematología Centro Médico Aspasia (Dra. Valvanera).   ->\n",
    "En seguimiento por Hematología [**Centro Médico Aspasia**] (Dra. [**Valvanera**]).\n",
    "Example:\n",
    "Control en Centro Salud Mental Reyes Católicos.   ->\n",
    "Control en [**Centro Salud Mental Reyes Católicos**].\n",
    "\n",
    "- Countries, territories, streets, etc\n",
    "Example:\n",
    "nacido en la República Italiana.   ->\n",
    "nacido en la [**República Italiana**].\n",
    "Example:\n",
    "ha estado viviendo en el Centro desde septiembre de 2008.   ->\n",
    "ha estado viviendo en el [**Centro**] desde [**septiembre de 2008**].\n",
    "Example:\n",
    "la dirección es Calle de Victor Hugo 39.   ->\n",
    "la dirección es [**Calle de Victor Hugo 39**].\n",
    "\n",
    "- Website URLs\n",
    "participar a través del siguiente enlace: https://www.donarsang.gencat.cat/covid19.   ->\n",
    "participar a través del siguiente enlace: [**https://www.donarsang.gencat.cat/covid19**].\n",
    "\n",
    "- Other sensitive information such as races, ethnicities, sexual orientation, dietary preferences, etc\n",
    "Example:\n",
    "raça blanca   ->\n",
    "[**raça blanca**]\n",
    "Example:\n",
    "Hsh\n",
    "[**Hsh**]\n",
    "Example:\n",
    "Vegetarià\n",
    "[**Vegetarià**]\n",
    "\n",
    "Do not comment anything else.\n",
    "Besides the anonymized attributes, provide the rest of the text exactly the same, including special characters and \\n symbols.\n",
    "Do not correct any typos or spacing errors at your discretion.\n",
    "For example, if the time is written as 31/12/2000-0 9:20:00 with incorrect spacing, do not return it corrected as 31/12/2000-09:20:00.\n",
    "Also, for example, if FLUTICASONA + AZELA STINA4 is written with incorrect spacing, do not return it corrected as FLUTICASONA + AZELASTINA 4.\n",
    "Only focus on the anonymization tasks I have specified, and ignore any typos or spacing errors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meddocan = \"\"\"\n",
    "Anota todos los datos de información personal médica que encuentres en este informe utilizando las guias meddocan a continuación:\n",
    "Nombres\n",
    "Datos geográficos\n",
    "Todos los elementos de las fechas\n",
    "Números telefónicos\n",
    "Números FAX\n",
    "Correos electrónicos\n",
    "Números de Seguridad Social\n",
    "Números de registros médicos\n",
    "Números de beneficiarios del plan de salud\n",
    "Números de cuenta\n",
    "Certificado / números de licencia\n",
    "Identificadores de vehículos y números de serie, incluidas placas\n",
    "Identificadores de dispositivo y números de serie\n",
    "URL web\n",
    "Direcciones de protocolo de Interne\n",
    "Identificadores biométricos (es decir, escaneo retiniano, huellas dactilar\n",
    "Fotos de cara completa e imágenes comparable\n",
    "Cualquier número de identificación único, característica o código\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    # (\"system\", \"You are an anonymization tool for identifying attributes in texts that can identify or quasi-identify a user. Whenever you find them, you should label them as follows: [**attribute|Generalization**].\"),\n",
    "    (\"system\", \"\"\"{system_prompt}\"\"\"),\n",
    "    (\"user\",    \"\"\"{text}\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_masked_carmen(name):\n",
    "    filename = f'./data/processed/txt/{name}'\n",
    "    filename_result = f'./data/processed/masked/{name}'\n",
    "    with open(filename, 'r') as archivo:\n",
    "        text = archivo.read()\n",
    "\n",
    "    with open(filename_result, 'r') as archivo:\n",
    "        text_masked = archivo.read()\n",
    "\n",
    "    return [text, text_masked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data  = []\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "llm_small_llama =   ChatBedrock(  \n",
    "                        model_id=MODELS[5],\n",
    "                        region_name='eu-west-2',\n",
    "                        model_kwargs=dict(\n",
    "                                    temperature = 0.1\n",
    "                                    ),\n",
    "                    )\n",
    "llm_big_llama   =   ChatBedrock(    \n",
    "                        model_id=MODELS[0],\n",
    "                        region_name='eu-west-2',\n",
    "                        model_kwargs=dict(\n",
    "                                    temperature = 0.1\n",
    "                                    ),\n",
    "                    )\n",
    "llm_haiku       =   ChatBedrock(    \n",
    "                        model_id=MODELS[1],\n",
    "                        region_name='eu-west-3',\n",
    "                        model_kwargs=dict(\n",
    "                                    temperature = 0.1\n",
    "                                    ),\n",
    "                    )\n",
    "llm_sonet       =   ChatBedrock(    \n",
    "                        model_id=MODELS[3],\n",
    "                        region_name='eu-west-3',\n",
    "                        model_kwargs=dict(\n",
    "                                    temperature = 0.1\n",
    "                                    ),\n",
    "                    )\n",
    "llm_mistral     =   ChatBedrock(\n",
    "                        model_id=MODELS[4],\n",
    "                        region_name='eu-west-2',\n",
    "                        model_kwargs=dict(\n",
    "                                    temperature = 0.1\n",
    "                                    ),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename, text):\n",
    "    with open(filename, 'w') as archivo:\n",
    "        archivo.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(name):\n",
    "    try:\n",
    "        os.mkdir(name)\n",
    "        print(f\"Folder '{name}' created successfully\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating the folder '{name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import warnings\n",
    "# import spacy\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", message=\"\\[W008\\] Evaluating Doc.similarity based on empty vectors\")\n",
    "# # Cargar el modelo de lenguaje en español mediano\n",
    "\n",
    "# nlp = spacy.load(\"es_core_news_md\")\n",
    "# # Función de similitud de embeddings\n",
    "# def embedding_similarity(str1, str2, threshold=0.8):\n",
    "#     doc1 = nlp(str1)\n",
    "#     doc2 = nlp(str2)\n",
    "#     similarity = doc1.similarity(doc2)\n",
    "#     return similarity >= threshold\n",
    "\n",
    "# def eliminar_adverbios_preposiciones_determinantes(texto):\n",
    "#     doc = nlp(texto)\n",
    "#     # Eliminar preposiciones (ADP) y determinantes (DET)\n",
    "#     tokens_filtrados = [token.text for token in doc if token.pos_ not in ('ADP', 'DET')]\n",
    "#     return ' '.join(tokens_filtrados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim(text_hoped, text_generated):\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w[\\w\\-/]*\\b\")\n",
    "    tfidf_matrix = vectorizer.fit_transform([text_hoped, text_generated])\n",
    "\n",
    "    try:\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    except: \n",
    "        return 0.0\n",
    "    return cosine_sim[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def levenshtein_distance(s1, s2, show_progress=True):\n",
    "    \"\"\"\n",
    "    Calcula la distancia de Levenshtein entre dos cadenas.\n",
    "\n",
    "    La distancia de Levenshtein es el número mínimo de operaciones de edición \n",
    "    (inserción, eliminación o sustitución de un carácter) necesarias para \n",
    "    transformar una cadena en otra.\n",
    "\n",
    "    Parámetros:\n",
    "        s1 (str): Primera cadena\n",
    "        s2 (str): Segunda cadena\n",
    "        show_progress (bool): Si es True, muestra una barra de progreso. \n",
    "                              Por defecto es False.\n",
    "    Retorna:\n",
    "        int: La distancia de Levenshtein entre s1 y s2\n",
    "    \"\"\"\n",
    "    # Usar tqdm solo si show_progress es True\n",
    "    iterable = tqdm(s1) if show_progress else s1\n",
    "\n",
    "    if len(s1) < len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(iterable):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_metrics(ground_truth, predictions):\n",
    "#     # Convertir arrays de ground_truth y predictions a listas de str\n",
    "#     ground_truth_processed = np.array([eliminar_adverbios_preposiciones_determinantes(str(item)) for item in ground_truth])\n",
    "#     predictions_processed = np.array([eliminar_adverbios_preposiciones_determinantes(str(item)) for item in predictions])\n",
    "\n",
    "#     # Crear matrices de similitud de coseno y embedding \n",
    "#     get_cos_sim_vectorized = np.vectorize(lambda gt, pred: get_cos_sim(str(gt), str(pred)))\n",
    "#     embedding_similarity_vectorized = np.vectorize(lambda gt, pred: embedding_similarity(str(gt), str(pred)))\n",
    "\n",
    "#     cosine_results = get_cos_sim_vectorized(ground_truth_processed[:, None], predictions_processed[None, :])\n",
    "#     embedding_results = embedding_similarity_vectorized(ground_truth_processed[:, None], predictions_processed[None, :])\n",
    "\n",
    "#     print(cosine_results)\n",
    "#     print(embedding_results)\n",
    "#     # Promediar las similitudes\n",
    "#     avg_similarities = (cosine_results + embedding_results) / 2\n",
    "\n",
    "#     # Determinar verdaderos positivos\n",
    "#     matches = avg_similarities > 0.5\n",
    "#     true_positives = np.sum(np.any(matches, axis=1))\n",
    "\n",
    "#     # Determinar falsos negativos\n",
    "#     false_negatives = len(ground_truth) - true_positives\n",
    "\n",
    "#     # Determinar falsos positivos\n",
    "#     predicted_matches = np.any(matches, axis=0)\n",
    "#     false_positives = len(predictions) - np.sum(predicted_matches)\n",
    "\n",
    "#     # Cálculo de métricas\n",
    "#     precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "#     recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "#     return precision, recall, f1\n",
    "\n",
    "def create_distance_matrix(str1, str2):\n",
    "    # Split the strings into words\n",
    "    str1 = str1.split(' ')\n",
    "    str2 = str2.split(' ')\n",
    "\n",
    "    # Initialize the matrix\n",
    "    m, n = len(str1), len(str2)\n",
    "    matrix = np.zeros((m + 1, n + 1), dtype=int)\n",
    "\n",
    "    # Fill the first row and first column\n",
    "    for i in range(1, m + 1):\n",
    "        matrix[i][0] = i\n",
    "    for j in range(1, n + 1):\n",
    "        matrix[0][j] = j\n",
    "\n",
    "    # Fill the matrix with edit distances\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                matrix[i][j] = matrix[i - 1][j - 1]\n",
    "            else:\n",
    "                matrix[i][j] = 1 + min(matrix[i - 1][j], matrix[i][j - 1], matrix[i - 1][j - 1])\n",
    "\n",
    "    # Create a DataFrame to include the words in the headers\n",
    "    df = pd.DataFrame(matrix, index=[\"\"] + str1, columns=[\"\"] + str2)\n",
    "    return df\n",
    "\n",
    "def trace_path(matrix, str1, str2):\n",
    "    m, n = len(str1), len(str2)\n",
    "    i, j = m, n\n",
    "    deleted = []\n",
    "    added = []\n",
    "    unchanged = []\n",
    "\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and str1[i - 1] == str2[j - 1]:\n",
    "            unchanged.append(str1[i - 1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and (j == 0 or matrix.iloc[i, j] == matrix.iloc[i - 1, j] + 1):\n",
    "            deleted.append(str1[i - 1])\n",
    "            i -= 1\n",
    "        elif j > 0 and (i == 0 or matrix.iloc[i, j] == matrix.iloc[i, j - 1] + 1):\n",
    "            added.append(str2[j - 1])\n",
    "            j -= 1\n",
    "        else:\n",
    "            deleted.append(str1[i - 1])\n",
    "            added.append(str2[j - 1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "\n",
    "    return deleted[::-1], added[::-1], unchanged[::-1]  # Reverse the lists to have the correct order\n",
    "\n",
    "\n",
    "def calc_precision_and_recall(list_of_ground_truth_labels, list_of_generated_labels):\n",
    "    # Check if inputs are lists\n",
    "    if not isinstance(list_of_ground_truth_labels, list) or not isinstance(list_of_generated_labels, list):\n",
    "        raise TypeError(\"Both inputs must be lists\")\n",
    "\n",
    "    str1 = ' '.join(list_of_ground_truth_labels)\n",
    "    str2 = ' '.join(list_of_generated_labels)\n",
    "\n",
    "    # Create the distance matrix\n",
    "    matrix = create_distance_matrix(str1, str2)\n",
    "\n",
    "    # Trace the minimum path and get the operations\n",
    "    added_words, deleted_words, unchanged_words = trace_path(matrix, str1.split(' '), str2.split(' '))\n",
    "\n",
    "    FN = len(added_words)\n",
    "    FP = len(deleted_words)\n",
    "    TP = len(unchanged_words)\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate(masked, generated):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        - masked (str): Ground_truth text\n",
    "        - generated(str): Text to be evaluated\n",
    "\n",
    "    Output:\n",
    "        - Precision, Recall and F1 (float)\n",
    "    \"\"\"\n",
    "    ground_truth = re.findall(r'\\[\\*\\*(.*?)\\*\\*\\]', masked)\n",
    "    predictions = re.findall(r'\\[\\*\\*(.*?)\\*\\*\\]', generated)\n",
    "\n",
    "    labels = [ground_truth, predictions]\n",
    "\n",
    "    return [calc_precision_and_recall(ground_truth, predictions), labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! METRICS\n",
    "\n",
    "def anonimized(llm=None, name_model=\"\"):\n",
    "    counter = 0\n",
    "    path = './data/processed/txt'\n",
    "    chain = prompt_template | llm | parser\n",
    "    list_data  = []\n",
    "    for filename in sorted(os.listdir(path)):\n",
    "        metrics_data = {}\n",
    "        metrics_data[\"filename\"] = filename\n",
    "        try:\n",
    "            [text, text_hoped] = get_text_and_masked_carmen(filename)\n",
    "            text_generated = chain.invoke({\"max_tokens\": 2000, \"system_prompt\": system_prompt, \"text\": text})\n",
    "            create_folder(f'data/anon/raw/{name_model}')\n",
    "            save_file(f'data/anon/raw/{name_model}/{filename}', text_generated)\n",
    "            # print(f\"THE COUNT IS {counter}\")\n",
    "            # print(f\"THE FILENAME IS {filename}\")\n",
    "            # print(text_generated)\n",
    "            # print(\" \")\n",
    "            # print(\"========================================\")\n",
    "            # print(text_hoped)\n",
    "            # print(\"========================================\")\n",
    "            # print(\"========================================\")\n",
    "            # print(\"========================================\")\n",
    "            # print(\" \")\n",
    "            # print(\" \")\n",
    "            # print(\" \")\n",
    "            [cal_met, labels] = evaluate(text_hoped, text_generated)    \n",
    "            cosine_sim = get_cos_sim(text_hoped, text_generated)\n",
    "            text_generated = text_generated.replace('[**', '').replace('**]', '')\n",
    "            text_hoped = text_hoped.replace('[**', '').replace('**]', '')\n",
    "            result = levenshtein_distance(text_generated, text_hoped[:len(text_generated)], show_progress=False)\n",
    "            metrics_data[\"precision\"] = cal_met[0]\n",
    "            metrics_data[\"recall\"] = cal_met[1]\n",
    "            metrics_data[\"f1\"] = cal_met[2]\n",
    "            metrics_data[\"cos\"] = cosine_sim\n",
    "            metrics_data[\"levenshtein\"] = result\n",
    "            metrics_data[\"labels hoped\"] = labels[0]\n",
    "            metrics_data[\"labels generated\"] = labels[1]\n",
    "            metrics_data[\"fail\"] = 0\n",
    "            if int(metrics_data[\"levenshtein\"]) == 0:\n",
    "                metrics_data[\"inv_levenshtein\"] = 1\n",
    "            else: \n",
    "                metrics_data[\"inv_levenshtein\"] = (1/metrics_data[\"levenshtein\"])\n",
    "            metrics_data[\"overall\"] = metrics_data[\"precision\"] + metrics_data[\"recall\"] +  metrics_data[\"f1\"] + metrics_data[\"cos\"] + metrics_data[\"inv_levenshtein\"]\n",
    "            list_data.append(metrics_data)\n",
    "            counter += 1\n",
    "            if counter > 0:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e) \n",
    "            metrics_data[\"fail\"] = 1\n",
    "        finally:\n",
    "            list_data.append(metrics_data)\n",
    "    list_data = pd.DataFrame(list_data)\n",
    "    list_data.to_csv(f'data/metrics/{name_model}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_small_llama = threading.Thread(target=anonimized, args=(llm_small_llama, \"small_llama\" ))\n",
    "thread_big_llama = threading.Thread(target=anonimized, args=(llm_big_llama, \"big_llama\" ))\n",
    "thread_haiku = threading.Thread(target=anonimized, args=(llm_haiku, \"haiku\" ))\n",
    "thread_sonet = threading.Thread(target=anonimized, args=(llm_sonet, \"sonet\" ))\n",
    "thread_mistral = threading.Thread(target=anonimized, args=(llm_mistral, \"mistral\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_small_llama.start()\n",
    "thread_big_llama.start()\n",
    "thread_haiku.start()\n",
    "thread_sonet.start()\n",
    "thread_mistral.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_big_llama.join()\n",
    "# thread_haiku.join()\n",
    "# thread_sonet.join()\n",
    "# thread_small_llama.join()\n",
    "# thread_mistral.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku_data = pd.read_csv('./data/metrics/haiku.csv')\n",
    "mistral_data = pd.read_csv('./data/metrics/mistral.csv')\n",
    "sonet_data = pd.read_csv('./data/metrics/sonet.csv')\n",
    "small_llama_data = pd.read_csv('./data/metrics/small_llama.csv')\n",
    "big_llama_data = pd.read_csv('./data/metrics/big_llama.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku_data = haiku_data[haiku_data['fail'] == 0]\n",
    "mistral_data = mistral_data[mistral_data['fail'] == 0]\n",
    "sonet_data = sonet_data[sonet_data['fail'] == 0]\n",
    "small_llama_data = small_llama_data[small_llama_data['fail'] == 0]\n",
    "big_llama_data = big_llama_data[big_llama_data['fail'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(haiku_data[\"precision\"].mean())\n",
    "print(mistral_data[\"precision\"].mean())\n",
    "print(sonet_data[\"precision\"].mean())\n",
    "print(small_llama_data[\"precision\"].mean())\n",
    "print(big_llama_data[\"precision\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(haiku_data[\"recall\"].mean())\n",
    "print(mistral_data[\"recall\"].mean())\n",
    "print(sonet_data[\"recall\"].mean())\n",
    "print(small_llama_data[\"recall\"].mean())\n",
    "print(big_llama_data[\"recall\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(haiku_data[\"f1\"].mean())\n",
    "print(mistral_data[\"f1\"].mean())\n",
    "print(sonet_data[\"f1\"].mean())\n",
    "print(small_llama_data[\"f1\"].mean())\n",
    "print(big_llama_data[\"f1\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(haiku_data[\"cos\"].mean())\n",
    "print(mistral_data[\"cos\"].mean())\n",
    "print(sonet_data[\"cos\"].mean())\n",
    "print(small_llama_data[\"cos\"].mean())\n",
    "print(big_llama_data[\"cos\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(haiku_data['levenshtein'].mean())\n",
    "print(mistral_data['levenshtein'].mean())\n",
    "print(sonet_data['levenshtein'].mean())\n",
    "print(small_llama_data['levenshtein'].mean())\n",
    "print(big_llama_data['levenshtein'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(haiku_data['inv_levenshtein'].mean())\n",
    "print(mistral_data['inv_levenshtein'].mean())\n",
    "print(sonet_data['inv_levenshtein'].mean())\n",
    "print(small_llama_data['inv_levenshtein'].mean())\n",
    "print(big_llama_data['inv_levenshtein'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(haiku_data['overall'].mean())\n",
    "print(mistral_data['overall'].mean())\n",
    "print(sonet_data['overall'].mean())\n",
    "print(small_llama_data['overall'].mean())\n",
    "print(big_llama_data['overall'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
