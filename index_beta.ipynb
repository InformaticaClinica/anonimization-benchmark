{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.context.llm_context import LLMContext\n",
    "from llm.strategy.haiku3_model import Haiku3Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"system\"] = \"You are a helpful AI assistant for travel tips and recommendations\"\n",
    "data[\"user\"] = \"What can you help me with?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anthropic.claude-3-haiku-20240307-v1:0\n"
     ]
    },
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the InvokeModel operation: \"claude-3-haiku-20240307\" is not supported on this API. Please use the Messages API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m context \u001b[38;5;241m=\u001b[39m LLMContext(Haiku3Model())\n\u001b[0;32m----> 2\u001b[0m text_generated \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/HospitalClinic/PLN-Radiologia/llama3/llm/context/llm_context.py:13\u001b[0m, in \u001b[0;36mLLMContext.generate_response\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/HospitalClinic/PLN-Radiologia/llama3/llm/strategy/haiku3_model.py:40\u001b[0m, in \u001b[0;36mHaiku3Model.generate_prompt\u001b[0;34m(self, model_prompt)\u001b[0m\n\u001b[1;32m     38\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_handler\u001b[38;5;241m.\u001b[39mformat_prompt(model_prompt)\n\u001b[1;32m     39\u001b[0m body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_body(prompt)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/HospitalClinic/PLN-Radiologia/llama3/llm/strategy/haiku3_model.py:28\u001b[0m, in \u001b[0;36mHaiku3Model.invoke_model\u001b[0;34m(self, body)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, body: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodelId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_contentType\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the InvokeModel operation: \"claude-3-haiku-20240307\" is not supported on this API. Please use the Messages API instead."
     ]
    }
   ],
   "source": [
    "context = LLMContext(Haiku3Model())\n",
    "text_generated = context.generate_response(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import spacy\n",
    "import numpy as np \n",
    "from colorama import init, Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(autoreset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"meta.llama3-70b-instruct-v1:0\", #Big Llama\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\", #Big Claude\n",
    "    \"anthropic.claude-3-5-sonnet-20240620-v1:0\", #Last Claude\n",
    "    \"anthropic.claude-3-sonnet-20240229-v1:0\", #Last middle Claude\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\", #Big Mistral\n",
    "    \"meta.llama3-8b-instruct-v1:0\" #Small Llama\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Devuelve solo el texto original con la identificación y etiquetación del texto con la información personal del paciente añadiendolo entre claudators, por ejemplo: [**arquitecto**], [**8/9/21**], [**padre**]. No comentes nada más\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poner -> Etiquetas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an anonymization tool in identifying attributes in texts that can identify or quasi-identify a user.\n",
    "Return only the original text with the identification and labeling of the patient's personal information by adding it between [** and **].\n",
    "Following are attributes that you must anonymize.\n",
    "\n",
    "- Names\n",
    "Example:\n",
    "En seguimiento por Hematología Centro Médico Aspasia (Dra. Valvanera).   ->\n",
    "En seguimiento por Hematología [**Centro Médico Aspasia**] (Dra. [**Valvanera**]).\n",
    "\n",
    "- Ages\n",
    "Example: \n",
    "Varón de 41 años.   ->\n",
    "[**Varón**] de [**41 años**].\n",
    "\n",
    "- Sexes\n",
    "Example: \n",
    "Varón de 41 años.   ->\n",
    "[**Varón**] de [**41 años**].\n",
    "\n",
    "- Professions\n",
    "Example: \n",
    "Trabaja como profesor.   ->\n",
    "Trabaja como [**profesor**].\n",
    "\n",
    "- Relatives\n",
    "Example: \n",
    "Vive con suegro y 2 yernos.   ->\n",
    "Vive con [**suegro**] y 2 [**yernos**].\n",
    "\n",
    "- Dates\n",
    "Example: \n",
    "ha estado viviendo en el Centro desde septiembre de 2008.   ->\n",
    "ha estado viviendo en el [**Centro**] desde [**septiembre de 2008**].\n",
    "\n",
    "- Phone numbers\n",
    "Example: \n",
    "contactando con el siguiente número de teléfono +50 88 078 68 49.   ->\n",
    "contactando con el siguiente número de teléfono [**+50 88 078 68 49**].\n",
    "\n",
    "- Identification numbers\n",
    "Example:\n",
    "El paciente otorga su consentimiento informado para participar en el estudio del protocolo WYX/8408/5545.   ->\n",
    "El paciente otorga su consentimiento informado para participar en el estudio del protocolo [**WYX/8408/5545.**]\n",
    "\n",
    "- Institutions, hospitals, health centers, etc\n",
    "Example: \n",
    "En seguimiento por Hematología Centro Médico Aspasia (Dra. Valvanera).   ->\n",
    "En seguimiento por Hematología [**Centro Médico Aspasia**] (Dra. [**Valvanera**]).\n",
    "Example:\n",
    "Control en Centro Salud Mental Reyes Católicos.   ->\n",
    "Control en [**Centro Salud Mental Reyes Católicos**].\n",
    "\n",
    "- Countries, territories, streets, etc\n",
    "Example:\n",
    "nacido en la República Italiana.   ->\n",
    "nacido en la [**República Italiana**].\n",
    "Example:\n",
    "ha estado viviendo en el Centro desde septiembre de 2008.   ->\n",
    "ha estado viviendo en el [**Centro**] desde [**septiembre de 2008**].\n",
    "Example:\n",
    "la dirección es Calle de Victor Hugo 39.   ->\n",
    "la dirección es [**Calle de Victor Hugo 39**].\n",
    "\n",
    "- Website URLs\n",
    "participar a través del siguiente enlace: https://www.donarsang.gencat.cat/covid19.   ->\n",
    "participar a través del siguiente enlace: [**https://www.donarsang.gencat.cat/covid19**].\n",
    "\n",
    "- Other sensitive information such as races, ethnicities, sexual orientation, dietary preferences, etc\n",
    "Example:\n",
    "raça blanca   ->\n",
    "[**raça blanca**]\n",
    "Example:\n",
    "Hsh\n",
    "[**Hsh**]\n",
    "Example:\n",
    "Vegetarià\n",
    "[**Vegetarià**]\n",
    "\n",
    "Do not comment anything else.\n",
    "Besides the anonymized attributes, provide the rest of the text exactly the same, including special characters and \\n symbols.\n",
    "Do not correct any typos or spacing errors at your discretion.\n",
    "For example, if the time is written as 31/12/2000-0 9:20:00 with incorrect spacing, do not return it corrected as 31/12/2000-09:20:00.\n",
    "Also, for example, if FLUTICASONA + AZELA STINA4 is written with incorrect spacing, do not return it corrected as FLUTICASONA + AZELASTINA 4.\n",
    "Only focus on the anonymization tasks I have specified, and ignore any typos or spacing errors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meddocan = \"\"\"\n",
    "Anota todos los datos de información personal médica que encuentres en este informe utilizando las guias meddocan a continuación:\n",
    "Nombres\n",
    "Datos geográficos\n",
    "Todos los elementos de las fechas\n",
    "Números telefónicos\n",
    "Números FAX\n",
    "Correos electrónicos\n",
    "Números de Seguridad Social\n",
    "Números de registros médicos\n",
    "Números de beneficiarios del plan de salud\n",
    "Números de cuenta\n",
    "Certificado / números de licencia\n",
    "Identificadores de vehículos y números de serie, incluidas placas\n",
    "Identificadores de dispositivo y números de serie\n",
    "URL web\n",
    "Direcciones de protocolo de Interne\n",
    "Identificadores biométricos (es decir, escaneo retiniano, huellas dactilar\n",
    "Fotos de cara completa e imágenes comparable\n",
    "Cualquier número de identificación único, característica o código\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    # (\"system\", \"You are an anonymization tool for identifying attributes in texts that can identify or quasi-identify a user. Whenever you find them, you should label them as follows: [**attribute|Generalization**].\"),\n",
    "    (\"system\", \"\"\"{system_prompt}\"\"\"),\n",
    "    (\"user\",    \"\"\"{text}\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_masked_carmen(name):\n",
    "    filename = f'./data/processed/txt/{name}'\n",
    "    filename_result = f'./data/processed/masked/{name}'\n",
    "    with open(filename, 'r') as archivo:\n",
    "        text = archivo.read()\n",
    "\n",
    "    with open(filename_result, 'r') as archivo:\n",
    "        text_masked = archivo.read()\n",
    "\n",
    "    return [text, text_masked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data  = []\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "llm_small_llama =   ChatBedrock(  \n",
    "                        model_id=MODELS[5],\n",
    "                        region_name='eu-west-2',\n",
    "                        model_kwargs=dict(\n",
    "                                    temperature = 0.1\n",
    "                                    ),\n",
    "                    )\n",
    "llm_big_llama   =   ChatBedrock(    \n",
    "                        model_id=MODELS[0],\n",
    "                        region_name='eu-west-2',\n",
    "                        model_kwargs=dict(\n",
    "                                    temperature = 0.1\n",
    "                                    ),\n",
    "                    )\n",
    "llm_haiku       =   ChatBedrock(    \n",
    "                        model_id=MODELS[1],\n",
    "                        region_name='eu-west-3',\n",
    "                        model_kwargs=dict(\n",
    "                                    temperature = 0.1\n",
    "                                    ),\n",
    "                    )\n",
    "llm_sonet       =   ChatBedrock(    \n",
    "                        model_id=MODELS[3],\n",
    "                        region_name='eu-west-3',\n",
    "                        model_kwargs=dict(\n",
    "                                    temperature = 0.1\n",
    "                                    ),\n",
    "                    )\n",
    "llm_mistral     =   ChatBedrock(\n",
    "                        model_id=MODELS[4],\n",
    "                        region_name='eu-west-2',\n",
    "                        model_kwargs=dict(\n",
    "                                    temperature = 0.1\n",
    "                                    ),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename, text):\n",
    "    with open(filename, 'w') as archivo:\n",
    "        archivo.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(name):\n",
    "    try:\n",
    "        os.mkdir(name)\n",
    "        print(f\"Folder '{name}' created successfully\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating the folder '{name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"\\[W008\\] Evaluating Doc.similarity based on empty vectors\")\n",
    "# Cargar el modelo de lenguaje en español mediano\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "# Función de similitud de embeddings\n",
    "def embedding_similarity(str1, str2, threshold=0.8):\n",
    "    doc1 = nlp(str1)\n",
    "    doc2 = nlp(str2)\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity >= threshold\n",
    "\n",
    "def eliminar_adverbios_preposiciones_determinantes(texto):\n",
    "    doc = nlp(texto)\n",
    "    # Eliminar preposiciones (ADP) y determinantes (DET)\n",
    "    tokens_filtrados = [token.text for token in doc if token.pos_ not in ('ADP', 'DET')]\n",
    "    return ' '.join(tokens_filtrados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim(text_hoped, text_generated):\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w[\\w\\-/]*\\b\")\n",
    "    tfidf_matrix = vectorizer.fit_transform([text_hoped, text_generated])\n",
    "\n",
    "    try:\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    except: \n",
    "        return 0.0\n",
    "    return cosine_sim[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def levenshtein_distance(s1, s2, show_progress=True):\n",
    "    \"\"\"\n",
    "    Calcula la distancia de Levenshtein entre dos cadenas.\n",
    "\n",
    "    La distancia de Levenshtein es el número mínimo de operaciones de edición \n",
    "    (inserción, eliminación o sustitución de un carácter) necesarias para \n",
    "    transformar una cadena en otra.\n",
    "\n",
    "    Parámetros:\n",
    "        s1 (str): Primera cadena\n",
    "        s2 (str): Segunda cadena\n",
    "        show_progress (bool): Si es True, muestra una barra de progreso. \n",
    "                              Por defecto es False.\n",
    "    Retorna:\n",
    "        int: La distancia de Levenshtein entre s1 y s2\n",
    "    \"\"\"\n",
    "    # Usar tqdm solo si show_progress es True\n",
    "    iterable = tqdm(s1) if show_progress else s1\n",
    "\n",
    "    if len(s1) < len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(iterable):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(ground_truth, predictions):\n",
    "    # Convertir arrays de ground_truth y predictions a listas de str\n",
    "    ground_truth_processed = np.array([eliminar_adverbios_preposiciones_determinantes(str(item)) for item in ground_truth])\n",
    "    predictions_processed = np.array([eliminar_adverbios_preposiciones_determinantes(str(item)) for item in predictions])\n",
    "\n",
    "    # Crear matrices de similitud de coseno y embedding \n",
    "    get_cos_sim_vectorized = np.vectorize(lambda gt, pred: get_cos_sim(str(gt), str(pred)))\n",
    "    embedding_similarity_vectorized = np.vectorize(lambda gt, pred: embedding_similarity(str(gt), str(pred)))\n",
    "\n",
    "    cosine_results = get_cos_sim_vectorized(ground_truth_processed[:, None], predictions_processed[None, :])\n",
    "    embedding_results = embedding_similarity_vectorized(ground_truth_processed[:, None], predictions_processed[None, :])\n",
    "\n",
    "    print(cosine_results)\n",
    "    print(embedding_results)\n",
    "    # Promediar las similitudes\n",
    "    avg_similarities = (cosine_results + embedding_results) / 2\n",
    "\n",
    "    # Determinar verdaderos positivos\n",
    "    matches = avg_similarities > 0.5\n",
    "    true_positives = np.sum(np.any(matches, axis=1))\n",
    "\n",
    "    # Determinar falsos negativos\n",
    "    false_negatives = len(ground_truth) - true_positives\n",
    "\n",
    "    # Determinar falsos positivos\n",
    "    predicted_matches = np.any(matches, axis=0)\n",
    "    false_positives = len(predictions) - np.sum(predicted_matches)\n",
    "\n",
    "    # Cálculo de métricas\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate(masked, generated):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        - masked (str): Ground_truth text\n",
    "        - generated(str): Text to be evaluated\n",
    "\n",
    "    Output:\n",
    "        - Precision, Recall and F1 (float)\n",
    "    \"\"\"\n",
    "    ground_truth = re.findall(r'\\[\\*\\*(.*?)\\*\\*\\]', masked)\n",
    "    predictions = re.findall(r'\\[\\*\\*(.*?)\\*\\*\\]', generated)\n",
    "    labels = [ground_truth, predictions]\n",
    "    \n",
    "    return [calc_metrics(ground_truth, predictions), labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! METRICS\n",
    "\n",
    "def anonimized(llm=None, name_model=\"\"):\n",
    "    counter = 0\n",
    "    path = './data/processed/txt'\n",
    "    chain = prompt_template | llm | parser\n",
    "    list_data  = []\n",
    "    for filename in sorted(os.listdir(path)):\n",
    "        metrics_data = {}\n",
    "        metrics_data[\"filename\"] = filename\n",
    "        try:\n",
    "            [text, text_hoped] = get_text_and_masked_carmen(filename)\n",
    "            text_generated = chain.invoke({\"max_tokens\": 2000, \"system_prompt\": system_prompt, \"text\": text})\n",
    "            create_folder(f'data/anon/raw/{name_model}')\n",
    "            save_file(f'data/anon/raw/{name_model}/{filename}', text_generated)\n",
    "            # print(f\"THE COUNT IS {counter}\")\n",
    "            # print(f\"THE FILENAME IS {filename}\")\n",
    "            # print(text_generated)\n",
    "            # print(\" \")\n",
    "            # print(\"========================================\")\n",
    "            # print(text_hoped)\n",
    "            # print(\"========================================\")\n",
    "            # print(\"========================================\")\n",
    "            # print(\"========================================\")\n",
    "            # print(\" \")\n",
    "            # print(\" \")\n",
    "            # print(\" \")\n",
    "            [cal_met, labels] = evaluate(text_hoped, text_generated)    \n",
    "            cosine_sim = get_cos_sim(text_hoped, text_generated)\n",
    "            text_generated = text_generated.replace('[**', '').replace('**]', '')\n",
    "            text_hoped = text_hoped.replace('[**', '').replace('**]', '')\n",
    "            result = levenshtein_distance(text_generated, text_hoped[:len(text_generated)], show_progress=False)\n",
    "            metrics_data[\"precision\"] = cal_met[0]\n",
    "            metrics_data[\"recall\"] = cal_met[1]\n",
    "            metrics_data[\"f1\"] = cal_met[2]\n",
    "            metrics_data[\"cos\"] = cosine_sim\n",
    "            metrics_data[\"levenshtein\"] = result\n",
    "            metrics_data[\"labels hoped\"] = labels[0]\n",
    "            metrics_data[\"labels generated\"] = labels[1]\n",
    "            metrics_data[\"fail\"] = 0\n",
    "            if int(metrics_data[\"levenshtein\"]) == 0:\n",
    "                metrics_data[\"inv_levenshtein\"] = 1\n",
    "            else: \n",
    "                metrics_data[\"inv_levenshtein\"] = (1/metrics_data[\"levenshtein\"])\n",
    "            metrics_data[\"overall\"] = metrics_data[\"precision\"] + metrics_data[\"recall\"] +  metrics_data[\"f1\"] + metrics_data[\"cos\"] + metrics_data[\"inv_levenshtein\"]\n",
    "            list_data.append(metrics_data)\n",
    "            counter += 1\n",
    "            if counter > 0:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e) \n",
    "            metrics_data[\"fail\"] = 1\n",
    "        finally:\n",
    "            list_data.append(metrics_data)\n",
    "    list_data = pd.DataFrame(list_data)\n",
    "    list_data.to_csv(f'data/metrics/{name_model}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_small_llama = threading.Thread(target=anonimized, args=(llm_small_llama, \"small_llama\" ))\n",
    "thread_big_llama = threading.Thread(target=anonimized, args=(llm_big_llama, \"big_llama\" ))\n",
    "thread_haiku = threading.Thread(target=anonimized, args=(llm_haiku, \"haiku\" ))\n",
    "thread_sonet = threading.Thread(target=anonimized, args=(llm_sonet, \"sonet\" ))\n",
    "thread_mistral = threading.Thread(target=anonimized, args=(llm_mistral, \"mistral\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_small_llama.start()\n",
    "thread_big_llama.start()\n",
    "thread_haiku.start()\n",
    "thread_sonet.start()\n",
    "thread_mistral.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'data/anon/raw/haiku' created successfully\n",
      "Folder 'data/anon/raw/small_llama' created successfully\n",
      "Folder 'data/anon/raw/mistral' created successfully\n",
      "Folder 'data/anon/raw/sonet' created successfully\n",
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.57973867 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "[[ True False False False False False False False False False]\n",
      " [False  True False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False  True]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]]\n",
      "Folder 'data/anon/raw/big_llama' created successfully\n",
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.57973867 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         1.         0.         1.         0.\n",
      "  0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         1.         0.         1.         0.\n",
      "  0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         1.         0.         1.         0.\n",
      "  0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         1.         0.         1.         0.\n",
      "  0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]]\n",
      "[[ True False False False False False False False False False False False\n",
      "  False False False False False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False]\n",
      " [False False False False  True False False False  True False  True False\n",
      "  False  True False False False]\n",
      " [False False False False False  True False False False  True False  True\n",
      "  False False  True False False]\n",
      " [False False False False False False  True False False False False False\n",
      "  False False False False False]\n",
      " [False False False False False False False  True False False False False\n",
      "  False False False False False]\n",
      " [False False False False  True False False False  True False  True False\n",
      "  False  True False False False]\n",
      " [False False False False False  True False False False  True False  True\n",
      "  False False  True False False]\n",
      " [False False False False  True False False False  True False  True False\n",
      "  False  True False False False]\n",
      " [False False False False False  True False False False  True False  True\n",
      "  False False  True False False]\n",
      " [False False False False  True False False False  True False  True False\n",
      "  False  True False False False]\n",
      " [False False False False False  True False False False  True False  True\n",
      "  False False  True False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False  True]\n",
      " [False False False False False False False False False False False False\n",
      "   True False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False  True False]]\n",
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.57973867 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         1.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         1.         0.         1.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         1.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         1.         0.         1.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         1.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         1.         0.         1.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         1.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         1.         0.         1.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "[[ True False False False False False False False False False False False]\n",
      " [False  True False False False False False False False False False False]\n",
      " [False False False False False False False False False False False False]\n",
      " [False False False  True False False False  True False  True False False]\n",
      " [False False False False  True False False False  True False  True False]\n",
      " [False False False False False  True False False False False False False]\n",
      " [False False False False False False  True False False False False False]\n",
      " [False False False  True False False False  True False  True False False]\n",
      " [False False False False  True False False False  True False  True False]\n",
      " [False False False  True False False False  True False  True False False]\n",
      " [False False False False  True False False False  True False  True False]\n",
      " [False False False  True False False False  True False  True False False]\n",
      " [False False False False  True False False False  True False  True False]\n",
      " [False False False False False False False False False False False False]\n",
      " [False False False False False False False False False False False  True]\n",
      " [False False False False False False False False False False False False]]\n",
      "[[0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.57973867 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  1.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  1.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  1.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  1.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.        ]]\n",
      "[[False  True False False False False False False False False False False\n",
      "  False False False False False False]\n",
      " [False False  True False False False False False False False False False\n",
      "  False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False]\n",
      " [False False False False  True False False False False  True False  True\n",
      "  False False  True False False False]\n",
      " [False False False False False  True False False False False  True False\n",
      "   True False False  True False False]\n",
      " [False False False False False False False  True False False False False\n",
      "  False False False False False False]\n",
      " [False False False False False False False False  True False False False\n",
      "  False False False False False False]\n",
      " [False False False False  True False False False False  True False  True\n",
      "  False False  True False False False]\n",
      " [False False False False False  True False False False False  True False\n",
      "   True False False  True False False]\n",
      " [False False False False  True False False False False  True False  True\n",
      "  False False  True False False False]\n",
      " [False False False False False  True False False False False  True False\n",
      "   True False False  True False False]\n",
      " [False False False False  True False False False False  True False  True\n",
      "  False False  True False False False]\n",
      " [False False False False False  True False False False False  True False\n",
      "   True False False  True False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False  True False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False  True False]]\n",
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.57973867 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  1.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  1.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  1.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  1.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.        ]]\n",
      "[[ True False False False False False False False False False False False\n",
      "  False False False False False False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False]\n",
      " [False False False  True False False False False False  True False  True\n",
      "  False False  True False False False]\n",
      " [False False False False  True False False False False False  True False\n",
      "   True False False  True False False]\n",
      " [False False False False False False  True False False False False False\n",
      "  False False False False False False]\n",
      " [False False False False False False False False  True False False False\n",
      "  False False False False False False]\n",
      " [False False False  True False False False False False  True False  True\n",
      "  False False  True False False False]\n",
      " [False False False False  True False False False False False  True False\n",
      "   True False False  True False False]\n",
      " [False False False  True False False False False False  True False  True\n",
      "  False False  True False False False]\n",
      " [False False False False  True False False False False False  True False\n",
      "   True False False  True False False]\n",
      " [False False False  True False False False False False  True False  True\n",
      "  False False  True False False False]\n",
      " [False False False False  True False False False False False  True False\n",
      "   True False False  True False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False  True False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False  True False]]\n"
     ]
    }
   ],
   "source": [
    "thread_big_llama.join()\n",
    "# thread_haiku.join()\n",
    "# thread_sonet.join()\n",
    "# thread_small_llama.join()\n",
    "# thread_mistral.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku_data = pd.read_csv('./data/metrics/haiku.csv')\n",
    "mistral_data = pd.read_csv('./data/metrics/mistral.csv')\n",
    "sonet_data = pd.read_csv('./data/metrics/sonet.csv')\n",
    "small_llama_data = pd.read_csv('./data/metrics/small_llama.csv')\n",
    "big_llama_data = pd.read_csv('./data/metrics/big_llama.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku_data = haiku_data[haiku_data['fail'] == 0]\n",
    "mistral_data = mistral_data[mistral_data['fail'] == 0]\n",
    "sonet_data = sonet_data[sonet_data['fail'] == 0]\n",
    "small_llama_data = small_llama_data[small_llama_data['fail'] == 0]\n",
    "big_llama_data = big_llama_data[big_llama_data['fail'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823529411764706\n",
      "0.9285714285714286\n",
      "0.7777777777777778\n",
      "0.3\n",
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data[\"precision\"].mean())\n",
    "print(mistral_data[\"precision\"].mean())\n",
    "print(sonet_data[\"precision\"].mean())\n",
    "print(small_llama_data[\"precision\"].mean())\n",
    "print(big_llama_data[\"precision\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n",
      "0.8125\n",
      "0.875\n",
      "0.1875\n",
      "0.875\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data[\"recall\"].mean())\n",
    "print(mistral_data[\"recall\"].mean())\n",
    "print(sonet_data[\"recall\"].mean())\n",
    "print(small_llama_data[\"recall\"].mean())\n",
    "print(big_llama_data[\"recall\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909092\n",
      "0.8666666666666666\n",
      "0.823529411764706\n",
      "0.2307692307692307\n",
      "0.823529411764706\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data[\"f1\"].mean())\n",
    "print(mistral_data[\"f1\"].mean())\n",
    "print(sonet_data[\"f1\"].mean())\n",
    "print(small_llama_data[\"f1\"].mean())\n",
    "print(big_llama_data[\"f1\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9888183329234088\n",
      "0.9254145309017976\n",
      "0.9888183329234088\n",
      "0.9891301841068816\n",
      "0.972083667603658\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data[\"cos\"].mean())\n",
    "print(mistral_data[\"cos\"].mean())\n",
    "print(sonet_data[\"cos\"].mean())\n",
    "print(small_llama_data[\"cos\"].mean())\n",
    "print(big_llama_data[\"cos\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data['levenshtein'].mean())\n",
    "print(mistral_data['levenshtein'].mean())\n",
    "print(sonet_data['levenshtein'].mean())\n",
    "print(small_llama_data['levenshtein'].mean())\n",
    "print(big_llama_data['levenshtein'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.2\n",
      "0.1666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data['inv_levenshtein'].mean())\n",
    "print(mistral_data['inv_levenshtein'].mean())\n",
    "print(sonet_data['inv_levenshtein'].mean())\n",
    "print(small_llama_data['inv_levenshtein'].mean())\n",
    "print(big_llama_data['inv_levenshtein'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.217762183190789\n",
      "4.033152626139893\n",
      "3.965125522465893\n",
      "1.907399414876112\n",
      "3.615057523812808\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data['overall'].mean())\n",
    "print(mistral_data['overall'].mean())\n",
    "print(sonet_data['overall'].mean())\n",
    "print(small_llama_data['overall'].mean())\n",
    "print(big_llama_data['overall'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
