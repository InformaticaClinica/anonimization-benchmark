{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import warnings\n",
    "import spacy\n",
    "import numpy as np \n",
    "from colorama import init, Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(autoreset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"meta.llama3-70b-instruct-v1:0\", #Big Llama\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\", #Big Claude\n",
    "    \"anthropic.claude-3-5-sonnet-20240620-v1:0\", #Last Claude\n",
    "    \"anthropic.claude-3-sonnet-20240229-v1:0\", #Last middle Claude\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\", #Big Mistral\n",
    "    \"meta.llama3-8b-instruct-v1:0\" #Small Llama\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"\"\"\n",
    "Devuelve el mismo texto sin eliminar nada, pero con la información personal que has decidido etiquetar dentro de claudátores, junto al nombre de la etiqueta correspondiente, por ejemplo: [**arquitecto|PROFESION**]\n",
    "No comentes nada más\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poner -> Etiquetas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meddocan = \"\"\"\n",
    "Anota todos los datos de información personal médica que encuentres en este informe utilizando las guias meddocan a continuación:\n",
    "Nombres\n",
    "Datos geográficos\n",
    "Todos los elementos de las fechas\n",
    "Números telefónicos\n",
    "Números FAX\n",
    "Correos electrónicos\n",
    "Números de Seguridad Social\n",
    "Números de registros médicos\n",
    "Números de beneficiarios del plan de salud\n",
    "Números de cuenta11.Certificado / números de licencia\n",
    "Identificadores de vehículos y números de serie, incluidas placas\n",
    "Identificadores de dispositivo y números de serie\n",
    "URL web\n",
    "Direcciones de protocolo de Interne\n",
    "Identificadores biométricos (es decir, escaneo retiniano, huellas dactilar\n",
    "Fotos de cara completa e imágenes comparable\n",
    "Cualquier número de identificación único, característica o código\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an anonimization tool in identifying attributes in texts that can identify or quasi-identify a user.\"),\n",
    "    (\"user\",    \"\"\"\n",
    "                    {prompt1} \n",
    "                    {text} \n",
    "                    {meddocan}\n",
    "                \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_masked_carmen(name):\n",
    "    filename = f'./data/processed/txt/{name}'\n",
    "    filename_result = f'./data/processed/masked/{name}'\n",
    "    with open(filename, 'r') as archivo:\n",
    "        text = archivo.read()\n",
    "\n",
    "    with open(filename_result, 'r') as archivo:\n",
    "        text_masked = archivo.read()\n",
    "\n",
    "    return [text, text_masked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data  = []\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "llm_small_llama =   ChatBedrock(  \n",
    "                            model_id=MODELS[5],\n",
    "                            region_name='eu-west-2',\n",
    "                            model_kwargs=dict(temperature=0.1),\n",
    "                    )\n",
    "llm_big_llama   =   ChatBedrock(    \n",
    "                            model_id=MODELS[0],\n",
    "                            region_name='eu-west-2',\n",
    "                            model_kwargs=dict(temperature=0.1),\n",
    "                    )\n",
    "llm_haiku       =   ChatBedrock(    \n",
    "                            model_id=MODELS[1],\n",
    "                            region_name='eu-west-3',\n",
    "                            model_kwargs=dict(temperature=0.1),\n",
    "                    )\n",
    "llm_sonet       =   ChatBedrock(    \n",
    "                            model_id=MODELS[3],\n",
    "                            region_name='eu-west-3',\n",
    "                            model_kwargs=dict(temperature=0.1),\n",
    "                    )\n",
    "llm_mistral     =   ChatBedrock(\n",
    "                        model_id=MODELS[4],\n",
    "                        region_name='eu-west-2',\n",
    "                        model_kwargs=dict(temperature=0.1)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename, text):\n",
    "    with open(filename, 'w') as archivo:\n",
    "        archivo.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(name):\n",
    "    try:\n",
    "        os.mkdir(name)\n",
    "        print(f\"Folder '{name}' created successfully\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating the folder '{name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"\\[W008\\] Evaluating Doc.similarity based on empty vectors\")\n",
    "# Cargar el modelo de lenguaje en español mediano\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "# Función de similitud de embeddings\n",
    "def embedding_similarity(str1, str2, threshold=0.8):\n",
    "    doc1 = nlp(str1)\n",
    "    doc2 = nlp(str2)\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity >= threshold\n",
    "\n",
    "def eliminar_adverbios_preposiciones_determinantes(texto):\n",
    "    doc = nlp(texto)\n",
    "    # Eliminar preposiciones (ADP) y determinantes (DET)\n",
    "    tokens_filtrados = [token.text for token in doc if token.pos_ not in ('ADP', 'DET')]\n",
    "    return ' '.join(tokens_filtrados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim(text_hoped, text_generated):\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w[\\w\\-/]*\\b\")\n",
    "    tfidf_matrix = vectorizer.fit_transform([text_hoped, text_generated])\n",
    "\n",
    "    try:\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    except: \n",
    "        return 0.0\n",
    "    return cosine_sim[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def levenshtein_distance(s1, s2, show_progress=True):\n",
    "    \"\"\"\n",
    "    Calcula la distancia de Levenshtein entre dos cadenas.\n",
    "\n",
    "    La distancia de Levenshtein es el número mínimo de operaciones de edición \n",
    "    (inserción, eliminación o sustitución de un carácter) necesarias para \n",
    "    transformar una cadena en otra.\n",
    "\n",
    "    Parámetros:\n",
    "        s1 (str): Primera cadena\n",
    "        s2 (str): Segunda cadena\n",
    "        show_progress (bool): Si es True, muestra una barra de progreso. \n",
    "                              Por defecto es False.\n",
    "    Retorna:\n",
    "        int: La distancia de Levenshtein entre s1 y s2\n",
    "    \"\"\"\n",
    "    # Usar tqdm solo si show_progress es True\n",
    "    iterable = tqdm(s1) if show_progress else s1\n",
    "\n",
    "    if len(s1) < len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(iterable):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(ground_truth, predictions):\n",
    "    # Convertir arrays de ground_truth y predictions a listas de str\n",
    "    ground_truth_processed = np.array([eliminar_adverbios_preposiciones_determinantes(str(item)) for item in ground_truth])\n",
    "    predictions_processed = np.array([eliminar_adverbios_preposiciones_determinantes(str(item)) for item in predictions])\n",
    "\n",
    "    # Crear matrices de similitud de coseno y embedding \n",
    "    get_cos_sim_vectorized = np.vectorize(lambda gt, pred: get_cos_sim(str(gt), str(pred)))\n",
    "    embedding_similarity_vectorized = np.vectorize(lambda gt, pred: embedding_similarity(str(gt), str(pred)))\n",
    "\n",
    "    cosine_results = get_cos_sim_vectorized(ground_truth_processed[:, None], predictions_processed[None, :])\n",
    "    embedding_results = embedding_similarity_vectorized(ground_truth_processed[:, None], predictions_processed[None, :])\n",
    "\n",
    "    # Promediar las similitudes\n",
    "    avg_similarities = (cosine_results + embedding_results) / 2\n",
    "\n",
    "    # Determinar verdaderos positivos\n",
    "    matches = avg_similarities > 0.5\n",
    "    true_positives = np.sum(np.any(matches, axis=1))\n",
    "\n",
    "    # Determinar falsos negativos\n",
    "    false_negatives = len(ground_truth) - true_positives\n",
    "\n",
    "    # Determinar falsos positivos\n",
    "    predicted_matches = np.any(matches, axis=0)\n",
    "    false_positives = len(predictions) - np.sum(predicted_matches)\n",
    "\n",
    "    # Cálculo de métricas\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate(masked, generated):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        - masked (str): Ground_truth text\n",
    "        - generated(str): Text to be evaluated\n",
    "\n",
    "    Output:\n",
    "        - Precision, Recall and F1 (float)\n",
    "    \"\"\"\n",
    "    ground_truth = re.findall(r'\\[\\*\\*(.*?)\\*\\*\\]', masked)\n",
    "    predictions = re.findall(r'\\[\\*\\*(.*?)\\*\\*\\]', generated)\n",
    "    labels = [ground_truth, predictions]\n",
    "    \n",
    "    return [calc_metrics(ground_truth, predictions), labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonimized(llm=None, name_model=\"\"):\n",
    "    counter = 0\n",
    "    path = './data/processed/txt'\n",
    "    chain = prompt_template | llm | parser\n",
    "    for filename in os.listdir(path):\n",
    "        [text, text_hoped] = get_text_and_masked_carmen(filename)\n",
    "        text_generated = chain.invoke({\"prompt1\":prompt1, \"text\": text, \"meddocan\": meddocan})\n",
    "        create_folder(f'data/anon/raw/{name_model}')\n",
    "        save_file(f'data/anon/raw/{name_model}/{filename}', text_generated)\n",
    "        [cal_met, labels] = evaluate(text_hoped, text_generated)    \n",
    "        cosine_sim = get_cos_sim(text_hoped, text_generated)\n",
    "        text_generated = text_generated.replace('[**', '').replace('**]', '')\n",
    "        text_hoped = text_hoped.replace('[**', '').replace('**]', '')\n",
    "        result = levenshtein_distance(text_generated, text_hoped[:len(text_generated)], show_progress=False)\n",
    "        metrics_data = {}\n",
    "        metrics_data[\"filename\"] = filename\n",
    "        metrics_data[\"precision\"] = cal_met[0]\n",
    "        metrics_data[\"recall\"] = cal_met[1]\n",
    "        metrics_data[\"f1\"] = cal_met[2]\n",
    "        metrics_data[\"cos\"] = cosine_sim\n",
    "        metrics_data[\"levenshtein\"] = result\n",
    "        metrics_data[\"labels hoped\"] = labels[0]\n",
    "        metrics_data[\"labels generated\"] = labels[1]\n",
    "        list_data.append(metrics_data)\n",
    "        counter += 1\n",
    "        if counter > 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_small_llama = threading.Thread(target=anonimized, args=(llm_small_llama, \"small_llama\" ))\n",
    "thread_big_llama = threading.Thread(target=anonimized, args=(llm_big_llama, \"big_llama\" ))\n",
    "thread_haiku = threading.Thread(target=anonimized, args=(llm_haiku, \"haiku\" ))\n",
    "thread_sonet = threading.Thread(target=anonimized, args=(llm_sonet, \"sonet\" ))\n",
    "thread_mistral = threading.Thread(target=anonimized, args=(llm_mistral, \"mistral\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/zq/7jhg214n1211qtx7dtxxljvc0000gn/T/ipykernel_53336/2501132566.py\", line 10, in anonimized\n",
      "  File \"/var/folders/zq/7jhg214n1211qtx7dtxxljvc0000gn/T/ipykernel_53336/3886728708.py\", line 47, in evaluate\n",
      "  File \"/var/folders/zq/7jhg214n1211qtx7dtxxljvc0000gn/T/ipykernel_53336/3886728708.py\", line 10, in calc_metrics\n",
      "  File \"/Users/petteraxcell/Library/Python/3.9/lib/python/site-packages/numpy/lib/function_base.py\", line 2328, in __call__\n",
      "    return self._vectorize_call(func=func, args=vargs)\n",
      "  File \"/Users/petteraxcell/Library/Python/3.9/lib/python/site-packages/numpy/lib/function_base.py\", line 2406, in _vectorize_call\n",
      "    ufunc, otypes = self._get_ufunc_and_otypes(func=func, args=args)\n",
      "  File \"/Users/petteraxcell/Library/Python/3.9/lib/python/site-packages/numpy/lib/function_base.py\", line 2362, in _get_ufunc_and_otypes\n",
      "    raise ValueError('cannot call `vectorize` on size 0 inputs '\n",
      "ValueError: cannot call `vectorize` on size 0 inputs unless `otypes` is set\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/zq/7jhg214n1211qtx7dtxxljvc0000gn/T/ipykernel_53336/2501132566.py\", line 10, in anonimized\n",
      "  File \"/var/folders/zq/7jhg214n1211qtx7dtxxljvc0000gn/T/ipykernel_53336/3886728708.py\", line 47, in evaluate\n",
      "  File \"/var/folders/zq/7jhg214n1211qtx7dtxxljvc0000gn/T/ipykernel_53336/3886728708.py\", line 10, in calc_metrics\n",
      "  File \"/Users/petteraxcell/Library/Python/3.9/lib/python/site-packages/numpy/lib/function_base.py\", line 2328, in __call__\n",
      "    return self._vectorize_call(func=func, args=vargs)\n",
      "  File \"/Users/petteraxcell/Library/Python/3.9/lib/python/site-packages/numpy/lib/function_base.py\", line 2406, in _vectorize_call\n",
      "    ufunc, otypes = self._get_ufunc_and_otypes(func=func, args=args)\n",
      "  File \"/Users/petteraxcell/Library/Python/3.9/lib/python/site-packages/numpy/lib/function_base.py\", line 2362, in _get_ufunc_and_otypes\n",
      "    raise ValueError('cannot call `vectorize` on size 0 inputs '\n",
      "ValueError: cannot call `vectorize` on size 0 inputs unless `otypes` is set\n"
     ]
    }
   ],
   "source": [
    "thread_small_llama.start()\n",
    "thread_big_llama.start()\n",
    "thread_haiku.start()\n",
    "thread_sonet.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
