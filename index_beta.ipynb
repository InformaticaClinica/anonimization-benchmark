{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import spacy\n",
    "import numpy as np \n",
    "from colorama import init, Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(autoreset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"meta.llama3-70b-instruct-v1:0\", #Big Llama\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\", #Big Claude\n",
    "    \"anthropic.claude-3-5-sonnet-20240620-v1:0\", #Last Claude\n",
    "    \"anthropic.claude-3-sonnet-20240229-v1:0\", #Last middle Claude\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\", #Big Mistral\n",
    "    \"meta.llama3-8b-instruct-v1:0\" #Small Llama\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Devuelve solo el texto original con la identificación y etiquetación del texto con la información personal del paciente añadiendolo entre claudators, por ejemplo: [**arquitecto**], [**8/9/21**], [**padre**]. No comentes nada más\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poner -> Etiquetas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meddocan = \"\"\"\n",
    "Anota todos los datos de información personal médica que encuentres en este informe utilizando las guias meddocan a continuación:\n",
    "Nombres\n",
    "Datos geográficos\n",
    "Todos los elementos de las fechas\n",
    "Números telefónicos\n",
    "Números FAX\n",
    "Correos electrónicos\n",
    "Números de Seguridad Social\n",
    "Números de registros médicos\n",
    "Números de beneficiarios del plan de salud\n",
    "Números de cuenta\n",
    "Certificado / números de licencia\n",
    "Identificadores de vehículos y números de serie, incluidas placas\n",
    "Identificadores de dispositivo y números de serie\n",
    "URL web\n",
    "Direcciones de protocolo de Interne\n",
    "Identificadores biométricos (es decir, escaneo retiniano, huellas dactilar\n",
    "Fotos de cara completa e imágenes comparable\n",
    "Cualquier número de identificación único, característica o código\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    # (\"system\", \"You are an anonymization tool for identifying attributes in texts that can identify or quasi-identify a user. Whenever you find them, you should label them as follows: [**attribute|Generalization**].\"),\n",
    "    (\"system\", \"You are an anonymization tool for identifying attributes in texts that can identify or quasi-identify a user.\"),\n",
    "    (\"user\",    \"\"\"\n",
    "                    {prompt1}\n",
    "                    {text}\n",
    "                    {meddocan}\n",
    "                \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_masked_carmen(name):\n",
    "    filename = f'./data/processed/txt/{name}'\n",
    "    filename_result = f'./data/processed/masked/{name}'\n",
    "    with open(filename, 'r') as archivo:\n",
    "        text = archivo.read()\n",
    "\n",
    "    with open(filename_result, 'r') as archivo:\n",
    "        text_masked = archivo.read()\n",
    "\n",
    "    return [text, text_masked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data  = []\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "llm_small_llama =   ChatBedrock(  \n",
    "                            model_id=MODELS[5],\n",
    "                            region_name='eu-west-2',\n",
    "                            model_kwargs=dict(temperature=0.1),\n",
    "                    )\n",
    "llm_big_llama   =   ChatBedrock(    \n",
    "                            model_id=MODELS[0],\n",
    "                            region_name='eu-west-2',\n",
    "                            model_kwargs=dict(temperature=0.1),\n",
    "                    )\n",
    "llm_haiku       =   ChatBedrock(    \n",
    "                            model_id=MODELS[1],\n",
    "                            region_name='eu-west-3',\n",
    "                            model_kwargs=dict(temperature=0.1),\n",
    "                    )\n",
    "llm_sonet       =   ChatBedrock(    \n",
    "                            model_id=MODELS[3],\n",
    "                            region_name='eu-west-3',\n",
    "                            model_kwargs=dict(temperature=0.1),\n",
    "                    )\n",
    "llm_mistral     =   ChatBedrock(\n",
    "                        model_id=MODELS[4],\n",
    "                        region_name='eu-west-2',\n",
    "                        model_kwargs=dict(temperature=0.1)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename, text):\n",
    "    with open(filename, 'w') as archivo:\n",
    "        archivo.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(name):\n",
    "    try:\n",
    "        os.mkdir(name)\n",
    "        print(f\"Folder '{name}' created successfully\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating the folder '{name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"\\[W008\\] Evaluating Doc.similarity based on empty vectors\")\n",
    "# Cargar el modelo de lenguaje en español mediano\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "# Función de similitud de embeddings\n",
    "def embedding_similarity(str1, str2, threshold=0.8):\n",
    "    doc1 = nlp(str1)\n",
    "    doc2 = nlp(str2)\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity >= threshold\n",
    "\n",
    "def eliminar_adverbios_preposiciones_determinantes(texto):\n",
    "    doc = nlp(texto)\n",
    "    # Eliminar preposiciones (ADP) y determinantes (DET)\n",
    "    tokens_filtrados = [token.text for token in doc if token.pos_ not in ('ADP', 'DET')]\n",
    "    return ' '.join(tokens_filtrados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim(text_hoped, text_generated):\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w[\\w\\-/]*\\b\")\n",
    "    tfidf_matrix = vectorizer.fit_transform([text_hoped, text_generated])\n",
    "\n",
    "    try:\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    except: \n",
    "        return 0.0\n",
    "    return cosine_sim[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def levenshtein_distance(s1, s2, show_progress=True):\n",
    "    \"\"\"\n",
    "    Calcula la distancia de Levenshtein entre dos cadenas.\n",
    "\n",
    "    La distancia de Levenshtein es el número mínimo de operaciones de edición \n",
    "    (inserción, eliminación o sustitución de un carácter) necesarias para \n",
    "    transformar una cadena en otra.\n",
    "\n",
    "    Parámetros:\n",
    "        s1 (str): Primera cadena\n",
    "        s2 (str): Segunda cadena\n",
    "        show_progress (bool): Si es True, muestra una barra de progreso. \n",
    "                              Por defecto es False.\n",
    "    Retorna:\n",
    "        int: La distancia de Levenshtein entre s1 y s2\n",
    "    \"\"\"\n",
    "    # Usar tqdm solo si show_progress es True\n",
    "    iterable = tqdm(s1) if show_progress else s1\n",
    "\n",
    "    if len(s1) < len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(iterable):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(ground_truth, predictions):\n",
    "    # Convertir arrays de ground_truth y predictions a listas de str\n",
    "    ground_truth_processed = np.array([eliminar_adverbios_preposiciones_determinantes(str(item)) for item in ground_truth])\n",
    "    predictions_processed = np.array([eliminar_adverbios_preposiciones_determinantes(str(item)) for item in predictions])\n",
    "\n",
    "    # Crear matrices de similitud de coseno y embedding \n",
    "    get_cos_sim_vectorized = np.vectorize(lambda gt, pred: get_cos_sim(str(gt), str(pred)))\n",
    "    embedding_similarity_vectorized = np.vectorize(lambda gt, pred: embedding_similarity(str(gt), str(pred)))\n",
    "\n",
    "    cosine_results = get_cos_sim_vectorized(ground_truth_processed[:, None], predictions_processed[None, :])\n",
    "    embedding_results = embedding_similarity_vectorized(ground_truth_processed[:, None], predictions_processed[None, :])\n",
    "\n",
    "    # Promediar las similitudes\n",
    "    avg_similarities = (cosine_results + embedding_results) / 2\n",
    "\n",
    "    # Determinar verdaderos positivos\n",
    "    matches = avg_similarities > 0.5\n",
    "    true_positives = np.sum(np.any(matches, axis=1))\n",
    "\n",
    "    # Determinar falsos negativos\n",
    "    false_negatives = len(ground_truth) - true_positives\n",
    "\n",
    "    # Determinar falsos positivos\n",
    "    predicted_matches = np.any(matches, axis=0)\n",
    "    false_positives = len(predictions) - np.sum(predicted_matches)\n",
    "\n",
    "    # Cálculo de métricas\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate(masked, generated):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        - masked (str): Ground_truth text\n",
    "        - generated(str): Text to be evaluated\n",
    "\n",
    "    Output:\n",
    "        - Precision, Recall and F1 (float)\n",
    "    \"\"\"\n",
    "    ground_truth = re.findall(r'\\[\\*\\*(.*?)\\*\\*\\]', masked)\n",
    "    predictions = re.findall(r'\\[\\*\\*(.*?)\\*\\*\\]', generated)\n",
    "    labels = [ground_truth, predictions]\n",
    "    \n",
    "    return [calc_metrics(ground_truth, predictions), labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! METRICS\n",
    "\n",
    "def anonimized(llm=None, name_model=\"\"):\n",
    "    counter = 0\n",
    "    path = './data/processed/txt'\n",
    "    chain = prompt_template | llm | parser\n",
    "    list_data  = []\n",
    "    for filename in os.listdir(path):\n",
    "        metrics_data = {}\n",
    "        metrics_data[\"filename\"] = filename\n",
    "        try:\n",
    "            [text, text_hoped] = get_text_and_masked_carmen(filename)\n",
    "            text_generated = chain.invoke({\"prompt1\":prompt1, \"text\": text, \"meddocan\": meddocan})\n",
    "            create_folder(f'data/anon/raw/{name_model}')\n",
    "            save_file(f'data/anon/raw/{name_model}/{filename}', text_generated)\n",
    "            # print(f\"THE COUNT IS {counter}\")\n",
    "            # print(f\"THE FILENAME IS {filename}\")\n",
    "            # print(text_generated)\n",
    "            # print(\" \")\n",
    "            # print(\"========================================\")\n",
    "            # print(text_hoped)\n",
    "            # print(\"========================================\")\n",
    "            # print(\"========================================\")\n",
    "            # print(\"========================================\")\n",
    "            # print(\" \")\n",
    "            # print(\" \")\n",
    "            # print(\" \")\n",
    "            [cal_met, labels] = evaluate(text_hoped, text_generated)    \n",
    "            cosine_sim = get_cos_sim(text_hoped, text_generated)\n",
    "            text_generated = text_generated.replace('[**', '').replace('**]', '')\n",
    "            text_hoped = text_hoped.replace('[**', '').replace('**]', '')\n",
    "            result = levenshtein_distance(text_generated, text_hoped[:len(text_generated)], show_progress=False)\n",
    "            metrics_data[\"precision\"] = cal_met[0]\n",
    "            metrics_data[\"recall\"] = cal_met[1]\n",
    "            metrics_data[\"f1\"] = cal_met[2]\n",
    "            metrics_data[\"cos\"] = cosine_sim\n",
    "            metrics_data[\"levenshtein\"] = result\n",
    "            metrics_data[\"labels hoped\"] = labels[0]\n",
    "            metrics_data[\"labels generated\"] = labels[1]\n",
    "            metrics_data[\"fail\"] = 0\n",
    "            metrics_data[\"inv_levenshtein\"] = (1/metrics_data[\"levenshtein\"])\n",
    "            metrics_data[\"overall\"] = metrics_data[\"precision\"] + metrics_data[\"recall\"] +  metrics_data[\"f1\"] + metrics_data[\"cos\"] + (1/metrics_data[\"levenshtein\"])\n",
    "            list_data.append(metrics_data)\n",
    "            counter += 1\n",
    "            if counter > 200:\n",
    "                break\n",
    "        except: \n",
    "            metrics_data[\"fail\"] = 1\n",
    "        finally:\n",
    "            list_data.append(metrics_data)\n",
    "    list_data = pd.DataFrame(list_data)\n",
    "    list_data.to_csv(f'data/metrics/{name_model}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_small_llama = threading.Thread(target=anonimized, args=(llm_small_llama, \"small_llama\" ))\n",
    "thread_big_llama = threading.Thread(target=anonimized, args=(llm_big_llama, \"big_llama\" ))\n",
    "thread_haiku = threading.Thread(target=anonimized, args=(llm_haiku, \"haiku\" ))\n",
    "thread_sonet = threading.Thread(target=anonimized, args=(llm_sonet, \"sonet\" ))\n",
    "thread_mistral = threading.Thread(target=anonimized, args=(llm_mistral, \"mistral\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_small_llama.start()\n",
    "thread_big_llama.start()\n",
    "thread_haiku.start()\n",
    "thread_sonet.start()\n",
    "thread_mistral.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_big_llama.join()\n",
    "thread_haiku.join()\n",
    "thread_sonet.join()\n",
    "thread_small_llama.join()\n",
    "thread_mistral.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku_data = pd.read_csv('./data/metrics/haiku.csv')\n",
    "mistral_data = pd.read_csv('./data/metrics/mistral.csv')\n",
    "sonet_data = pd.read_csv('./data/metrics/sonet.csv')\n",
    "small_llama_data = pd.read_csv('./data/metrics/small_llama.csv')\n",
    "big_llama_data = pd.read_csv('./data/metrics/big_llama.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku_data = haiku_data[haiku_data['fail'] == 0]\n",
    "mistral_data = mistral_data[mistral_data['fail'] == 0]\n",
    "sonet_data = sonet_data[sonet_data['fail'] == 0]\n",
    "small_llama_data = small_llama_data[small_llama_data['fail'] == 0]\n",
    "big_llama_data = big_llama_data[big_llama_data['fail'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# haiku_data['inv_levenshtein'] = 1 / haiku_data['levenshtein']\n",
    "# mistral_data['inv_levenshtein'] = 1 / mistral_data['levenshtein']\n",
    "# sonet_data['inv_levenshtein'] = 1 / sonet_data['levenshtein']\n",
    "# small_llama_data['inv_levenshtein'] = 1 / small_llama_data['levenshtein']\n",
    "# big_llama_data['inv_levenshtein'] = 1 / big_llama_data['levenshtein']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45328341410863315\n",
      "0.4292630365273149\n",
      "0.45910017827377175\n",
      "0.3384352570845056\n",
      "0.579338334408687\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data[\"precision\"].mean())\n",
    "print(mistral_data[\"precision\"].mean())\n",
    "print(sonet_data[\"precision\"].mean())\n",
    "print(small_llama_data[\"precision\"].mean())\n",
    "print(big_llama_data[\"precision\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5989834224898577\n",
      "0.5642868859644175\n",
      "0.7079265508607363\n",
      "0.5241150296803521\n",
      "0.5912571004750007\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data[\"recall\"].mean())\n",
    "print(mistral_data[\"recall\"].mean())\n",
    "print(sonet_data[\"recall\"].mean())\n",
    "print(small_llama_data[\"recall\"].mean())\n",
    "print(big_llama_data[\"recall\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46160567989176526\n",
      "0.4202341543271648\n",
      "0.5057010564222725\n",
      "0.34849754035579456\n",
      "0.5371648155929787\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data[\"f1\"].mean())\n",
    "print(mistral_data[\"f1\"].mean())\n",
    "print(sonet_data[\"f1\"].mean())\n",
    "print(small_llama_data[\"f1\"].mean())\n",
    "print(big_llama_data[\"f1\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4375815691092509\n",
      "0.821997257456751\n",
      "0.461771547319193\n",
      "0.8516311609025682\n",
      "0.9184920071079429\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data[\"cos\"].mean())\n",
    "print(mistral_data[\"cos\"].mean())\n",
    "print(sonet_data[\"cos\"].mean())\n",
    "print(small_llama_data[\"cos\"].mean())\n",
    "print(big_llama_data[\"cos\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.03482587064677\n",
      "135.64676616915423\n",
      "129.07960199004975\n",
      "84.39800995024876\n",
      "81.5273631840796\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data['levenshtein'].mean())\n",
    "print(mistral_data['levenshtein'].mean())\n",
    "print(sonet_data['levenshtein'].mean())\n",
    "print(small_llama_data['levenshtein'].mean())\n",
    "print(big_llama_data['levenshtein'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08145141147578322\n",
      "0.23123834634305143\n",
      "0.073671099661234\n",
      "0.08056951986780253\n",
      "0.12811058063807035\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data['inv_levenshtein'].mean())\n",
    "print(mistral_data['inv_levenshtein'].mean())\n",
    "print(sonet_data['inv_levenshtein'].mean())\n",
    "print(small_llama_data['inv_levenshtein'].mean())\n",
    "print(big_llama_data['inv_levenshtein'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.03290549707529\n",
      "2.4670196806186997\n",
      "2.208170432537208\n",
      "2.143248507891023\n",
      "2.7543628382226792\n"
     ]
    }
   ],
   "source": [
    "print(haiku_data['overall'].mean())\n",
    "print(mistral_data['overall'].mean())\n",
    "print(sonet_data['overall'].mean())\n",
    "print(small_llama_data['overall'].mean())\n",
    "print(big_llama_data['overall'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
