{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.context.llm_context import LLMContext\n",
    "from llm.strategy.big_llama_model import BigLlamaModel\n",
    "from llm.strategy.small_llama_model import SmallLlamaModel\n",
    "from llm.strategy.big_mistral_model import BigMistralModel\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './data/processed/txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(name):\n",
    "    try:\n",
    "        os.mkdir(name)\n",
    "        print(f\"Folder '{name}' created successfully\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating the folder '{name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename, text):\n",
    "    with open(filename, 'w') as archivo:\n",
    "        archivo.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_masked_carmen(name):\n",
    "    filename = f'./data/processed/txt/{name}'\n",
    "    filename_result = f'./data/processed/masked/{name}'\n",
    "    with open(filename, 'r') as archivo:\n",
    "        text = archivo.read()\n",
    "\n",
    "    with open(filename_result, 'r') as archivo:\n",
    "        text_masked = archivo.read()\n",
    "\n",
    "    return [text, text_masked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_metrics(metrics_data, precision, recall, f1, cosine_sim, levenshtein_distance, labels, name_model):\n",
    "    metrics_data[\"precision\"] = precision\n",
    "    metrics_data[\"recall\"] = recall\n",
    "    metrics_data[\"f1\"] = f1\n",
    "    metrics_data[\"cos\"] = cosine_sim\n",
    "    metrics_data[\"levenshtein\"] = levenshtein_distance\n",
    "    metrics_data[\"name_model\"] = name_model\n",
    "    metrics_data[\"ground_truth\"] = labels[0]\n",
    "    metrics_data[\"generated\"] = labels[1]\n",
    "    if int(metrics_data[\"levenshtein\"]) == 0:\n",
    "        metrics_data[\"inv_levenshtein\"] = 1\n",
    "    else: \n",
    "        metrics_data[\"inv_levenshtein\"] = (1/metrics_data[\"levenshtein\"])\n",
    "        metrics_data[\"overall\"] = metrics_data[\"precision\"] + metrics_data[\"recall\"] +  metrics_data[\"f1\"] + metrics_data[\"cos\"] + metrics_data[\"inv_levenshtein\"]\n",
    "    return metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(name_model, list_data):\n",
    "    list_data = pd.DataFrame(list_data)\n",
    "    list_data.to_csv(f'data/metrics/{name_model}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"\\[W008\\] Evaluating Doc.similarity based on empty vectors\")\n",
    "nlp = spacy.load(\"es_core_news_md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_similarity(str1, str2, threshold=0.8):\n",
    "    doc1 = nlp(str1)\n",
    "    doc2 = nlp(str2)\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_adverbios_preposiciones_determinantes(texto):\n",
    "    doc = nlp(texto)\n",
    "    # Eliminar preposiciones (ADP) y determinantes (DET)\n",
    "    tokens_filtrados = [token.text for token in doc if token.pos_ not in ('ADP', 'DET')]\n",
    "    return ' '.join(tokens_filtrados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2, show_progress=True):\n",
    "    \"\"\"\n",
    "    Calcula la distancia de Levenshtein entre dos cadenas.\n",
    "\n",
    "    La distancia de Levenshtein es el número mínimo de operaciones de edición \n",
    "    (inserción, eliminación o sustitución de un carácter) necesarias para \n",
    "    transformar una cadena en otra.\n",
    "\n",
    "    Parámetros:\n",
    "        s1 (str): Primera cadena\n",
    "        s2 (str): Segunda cadena\n",
    "        show_progress (bool): Si es True, muestra una barra de progreso. \n",
    "                              Por defecto es False.\n",
    "    Retorna:\n",
    "        int: La distancia de Levenshtein entre s1 y s2\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Usar tqdm solo si show_progress es True\n",
    "    iterable = tqdm(s1) if show_progress else s1\n",
    "\n",
    "    if len(s1) < len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(iterable):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim(text_hoped, text_generated):\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w[\\w\\-/]*\\b\")\n",
    "    tfidf_matrix = vectorizer.fit_transform([text_hoped, text_generated])\n",
    "\n",
    "    try:\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    except: \n",
    "        return 0.0\n",
    "    return cosine_sim[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(ground_truth, predictions):\n",
    "    # Convertir arrays de ground_truth y predictions a listas de str\n",
    "    ground_truth_processed = np.array([eliminar_adverbios_preposiciones_determinantes(str(item)) for item in ground_truth])\n",
    "    predictions_processed = np.array([eliminar_adverbios_preposiciones_determinantes(str(item)) for item in predictions])\n",
    "\n",
    "    # Crear matrices de similitud de coseno y embedding \n",
    "    get_cos_sim_vectorized = np.vectorize(lambda gt, pred: get_cos_sim(str(gt), str(pred)))\n",
    "    embedding_similarity_vectorized = np.vectorize(lambda gt, pred: embedding_similarity(str(gt), str(pred)))\n",
    "\n",
    "    cosine_results = get_cos_sim_vectorized(ground_truth_processed[:, None], predictions_processed[None, :])\n",
    "    embedding_results = embedding_similarity_vectorized(ground_truth_processed[:, None], predictions_processed[None, :])\n",
    "\n",
    "    # Promediar las similitudes\n",
    "    avg_similarities = (cosine_results + embedding_results) / 2\n",
    "\n",
    "    # Determinar verdaderos positivos\n",
    "    matches = avg_similarities > 0.5\n",
    "    true_positives = np.sum(np.any(matches, axis=1))\n",
    "\n",
    "    # Determinar falsos negativos\n",
    "    false_negatives = len(ground_truth) - true_positives\n",
    "\n",
    "    # Determinar falsos positivos\n",
    "    predicted_matches = np.any(matches, axis=0)\n",
    "    false_positives = len(predictions) - np.sum(predicted_matches)\n",
    "\n",
    "    # Cálculo de métricas\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(masked, generated):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        - masked (str): Ground_truth text\n",
    "        - generated(str): Text to be evaluated\n",
    "\n",
    "    Output:\n",
    "        - Precision, Recall and F1 (float)\n",
    "    \"\"\"\n",
    "    ground_truth = re.findall(r'\\[\\*\\*(.*?)\\*\\*\\]', masked)\n",
    "    predictions = re.findall(r'\\[\\*\\*(.*?)\\*\\*\\]', generated)\n",
    "    labels = [ground_truth, predictions]\n",
    "    \n",
    "    return [calc_metrics(ground_truth, predictions), labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(text_hoped, text_generated):\n",
    "    cosine_sim = get_cos_sim(text_hoped, text_generated)\n",
    "    [cal_met, labels] = evaluate(text_hoped, text_generated)\n",
    "    print(labels)\n",
    "    text_generated = text_generated.replace('[**', '').replace('**]', '')\n",
    "    text_hoped = text_hoped.replace('[**', '').replace('**]', '')\n",
    "    result = levenshtein_distance(text_generated, text_hoped[:len(text_generated)], show_progress=False)\n",
    "    return cosine_sim, cal_met, result, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special_characters(text):\n",
    "    # Define the pattern to match special characters including . / and -\n",
    "    pattern = r'[!@#$%^&*()_+={}\\[\\]:;\"\\'<>,?\\\\|`~./-]'\n",
    "    \n",
    "    # Replace the matched characters with a space\n",
    "    result = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def evaluate2(masked,generated):\n",
    "    masked=masked.replace('\\n','')\n",
    "    generated=generated.replace('\\n','')\n",
    "\n",
    "    ground_truth_matches = re.finditer(r'\\[\\*\\*(.*?)\\*\\*\\]', masked)\n",
    "    ground_truth_positions = {}\n",
    "    cnt=0\n",
    "    for match in ground_truth_matches:\n",
    "        start = match.start(1)-(cnt*2+1)*3  # start of the group (excluding [**)\n",
    "        end = match.end(1)-(cnt*2+1)*3\n",
    "        cnt+=1# end of the group (excluding **])\n",
    "        ground_truth_positions[(start, end)] = replace_special_characters(match.group(1))\n",
    "\n",
    "    predictions_matches = re.finditer(r'\\[\\*\\*(.*?)\\*\\*\\]', generated)\n",
    "    predictions_positions = {}\n",
    "    cnt=0\n",
    "    for match in predictions_matches:\n",
    "        start = match.start(1)-(cnt*2+1)*3  # start of the group (excluding [**)\n",
    "        end = match.end(1)-(cnt*2+1)*3\n",
    "        cnt+=1# end of the group (excluding **])\n",
    "        predictions_positions[(start, end)] = replace_special_characters(match.group(1))\n",
    "\n",
    "    totalwordcnt_ground_truth = len(ground_truth_positions)\n",
    "    score_total=0\n",
    "    for pos_g in ground_truth_positions:\n",
    "        for pos_p in predictions_positions:\n",
    "            if (pos_p[0]<=pos_g[0] and pos_p[1]>=pos_g[1]) or (pos_p[0]>=pos_g[0] and pos_p[1]<=pos_g[1]):\n",
    "                score_temp = partial_score(ground_truth_positions[pos_g],predictions_positions[pos_p])\n",
    "                score_total += score_temp\n",
    "\n",
    "    score_total = score_total/totalwordcnt_ground_truth\n",
    "    recall = score_total\n",
    "\n",
    "    totalwordcnt_predictions = len(predictions_positions)\n",
    "    score_total=0\n",
    "    for pos_p in predictions_positions:\n",
    "        for pos_g in ground_truth_positions:\n",
    "            if (pos_g[0]<=pos_p[0] and pos_g[1]>=pos_p[1]) or (pos_g[0]>=pos_p[0] and pos_g[1]<=pos_p[1]):\n",
    "                score_temp = partial_score(predictions_positions[pos_p],ground_truth_positions[pos_g])\n",
    "                score_total += score_temp\n",
    "\n",
    "    score_total = score_total/totalwordcnt_predictions\n",
    "    precision = score_total\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonimized(llm=None, name_model=\"\", data=None):\n",
    "    counter = 0\n",
    "    context = LLMContext(llm)\n",
    "    list_data  = []\n",
    "    for filename in sorted(os.listdir(PATH)):\n",
    "        metrics_data = {}\n",
    "        metrics_data[\"filename\"] = filename\n",
    "        try:\n",
    "            [text, text_hoped] = get_text_and_masked_carmen(filename)\n",
    "            data[\"user\"] = text\n",
    "            text_generated = context.generate_response(data)\n",
    "            \n",
    "            \n",
    "            # #### Second metrics\n",
    "            # precision, recall, f1 = evaluate2(text_hoped, text_generated)\n",
    "            # print(f\"precision: {precision}, recall: {recall}, f1: {f1}\")\n",
    "            # #### Second metrics\n",
    "\n",
    "            create_folder(f'data/anon/raw/{name_model}')\n",
    "            save_file(f'data/anon/raw/{name_model}/{filename}', text_generated)\n",
    "            cosine_sim, cal_met, result, labels = calculate_metrics(text_hoped, text_generated)\n",
    "            metrics_data = store_metrics(metrics_data, cal_met[0],cal_met[1], cal_met[2], cosine_sim, result, labels, name_model)\n",
    "            list_data.append(metrics_data)\n",
    "            counter += 1\n",
    "            if counter > 1:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e) \n",
    "            metrics_data[\"fail\"] = 1\n",
    "        finally:\n",
    "            list_data.append(metrics_data)\n",
    "    save_metrics(name_model, list_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"system\"] = \"\"\"\n",
    "You are an anonymization tool in identifying attributes in texts that can identify or quasi-identify a user.\n",
    "Return only the original text with the identification and labeling of the patient's personal information by adding it between [** and **].\n",
    "Following are attributes that you must anonymize.\n",
    "\n",
    "- Names\n",
    "Example:\n",
    "En seguimiento por Hematología Centro Médico Aspasia (Dra. Valvanera).   ->\n",
    "En seguimiento por Hematología [**Centro Médico Aspasia**] (Dra. [**Valvanera**]).\n",
    "\n",
    "- Family number:\n",
    "2 yernos ->\n",
    "[**2 yernos**]\n",
    "\n",
    "- Ages\n",
    "Example: \n",
    "Varón de 41 años.   ->\n",
    "[**Varón**] de [**41 años**].\n",
    "\n",
    "- Sexes\n",
    "Example: \n",
    "Varón de 41 años.   ->\n",
    "[**Varón**] de [**41 años**].\n",
    "\n",
    "- Professions\n",
    "Example: \n",
    "Trabaja como profesor.   ->\n",
    "Trabaja como [**profesor**].\n",
    "\n",
    "- Relatives\n",
    "Example: \n",
    "Vive con suegro y 2 yernos.   ->\n",
    "Vive con [**suegro**] y 2 [**yernos**].\n",
    "\n",
    "- Dates\n",
    "Example: \n",
    "ha estado viviendo en el Centro desde septiembre de 2008.   ->\n",
    "ha estado viviendo en el [**Centro**] desde [**septiembre de 2008**].\n",
    "\n",
    "- Phone numbers\n",
    "Example: \n",
    "contactando con el siguiente número de teléfono +50 88 078 68 49.   ->\n",
    "contactando con el siguiente número de teléfono [**+50 88 078 68 49**].\n",
    "\n",
    "- Identification numbers\n",
    "Example:\n",
    "El paciente otorga su consentimiento informado para participar en el estudio del protocolo WYX/8408/5545.   ->\n",
    "El paciente otorga su consentimiento informado para participar en el estudio del protocolo [**WYX/8408/5545.**]\n",
    "\n",
    "- Institutions, hospitals, health centers, etc\n",
    "Example: \n",
    "En seguimiento por Hematología Centro Médico Aspasia (Dra. Valvanera).   ->\n",
    "En seguimiento por Hematología [**Centro Médico Aspasia**] (Dra. [**Valvanera**]).\n",
    "Example:\n",
    "Control en Centro Salud Mental Reyes Católicos.   ->\n",
    "Control en [**Centro Salud Mental Reyes Católicos**].\n",
    "\n",
    "- Countries, territories, streets, etc\n",
    "Example:\n",
    "nacido en la República Italiana.   ->\n",
    "nacido en la [**República Italiana**].\n",
    "Example:\n",
    "ha estado viviendo en el Centro desde septiembre de 2008.   ->\n",
    "ha estado viviendo en el [**Centro**] desde [**septiembre de 2008**].\n",
    "Example:\n",
    "la dirección es Calle de Victor Hugo 39.   ->\n",
    "la dirección es [**Calle de Victor Hugo 39**].\n",
    "\n",
    "- Website URLs\n",
    "participar a través del siguiente enlace: https://www.donarsang.gencat.cat/covid19.   ->\n",
    "participar a través del siguiente enlace: [**https://www.donarsang.gencat.cat/covid19**].\n",
    "\n",
    "- Other sensitive information such as races, ethnicities, sexual orientation, dietary preferences, etc\n",
    "Example:\n",
    "raça blanca   ->\n",
    "[**raça blanca**]\n",
    "Example:\n",
    "Hsh\n",
    "[**Hsh**]\n",
    "Example:\n",
    "Vegetarià\n",
    "[**Vegetarià**]\n",
    "\n",
    "Do not comment anything else.\n",
    "Besides the anonymized attributes, provide the rest of the text exactly the same, including special characters and \\n symbols.\n",
    "Do not correct any typos or spacing errors at your discretion.\n",
    "For example, if the time is written as 31/12/2000-0 9:20:00 with incorrect spacing, do not return it corrected as 31/12/2000-09:20:00.\n",
    "Also, for example, if FLUTICASONA + AZELA STINA4 is written with incorrect spacing, do not return it corrected as FLUTICASONA + AZELASTINA 4.\n",
    "Only focus on the anonymization tasks I have specified, and ignore any typos or spacing errors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3118"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"system\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'data/anon/raw/big_llama3' created successfully\n",
      "[['hilandero', 'suegro', '2 yernos', '8/9/21', '7/9/21', '976028134', '983425634', '8/9/21', '7/9/21', '8/9/21', '7/9/21', '8/9/21', '7/9/21', 'Salud laboral', '15/09', '10.09.2021'], ['hilandero', 'suegro', 'yernos', '8/9/21', '7/9/21', 'HDOM', '976028134', '983425634', '8/9/21', '7/9/21', '8/9/21', '7/9/21', '15/09', '8/9/21', '7/9/21', '10.09.2021', 'HDOM', 'Salud laboral']]\n",
      "[['20/05', 'Centro COVID', 'suegros', '27/04/2011', '29/04/2011', 'Hospital COVID', '3/05/2011', '30/04', '7/05', '7/05', '18/05', 'Centro COVID', 'suegros', '27/04/2011', '29/04/2011', '3/05/2011', '30/04', '7/05', '7/05', 'WYX/8408/5545'], ['20/05', 'Centro COVID', 'suegros', '27/04/2011', '29/04/2011', 'Hospital COVID', '3/05/2011', '30/04', '7/05', '7/05', '18/05', 'Centro COVID', 'suegros', '27/04/2011', '29/04/2011', 'Hospital SALUT', '3/05/2011', '30/04', '7/05', '7/05', 'WYX/8408/5545']]\n"
     ]
    }
   ],
   "source": [
    "anonimized(llm=BigLlamaModel(), name_model=\"big_llama3\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'data/anon/raw/small_llama3' created successfully\n",
      "[['hilandero', 'suegro', '2 yernos', '8/9/21', '7/9/21', '976028134', '983425634', '8/9/21', '7/9/21', '8/9/21', '7/9/21', '8/9/21', '7/9/21', 'Salud laboral', '15/09', '10.09.2021'], ['hilandero', 'suegro', 'yernos', 'rinitis alergica', 'Teléfono HDOM', '976028134', '983425634', 'HDOM', 'Salud laboral']]\n",
      "[['20/05', 'Centro COVID', 'suegros', '27/04/2011', '29/04/2011', 'Hospital COVID', '3/05/2011', '30/04', '7/05', '7/05', '18/05', 'Centro COVID', 'suegros', '27/04/2011', '29/04/2011', '3/05/2011', '30/04', '7/05', '7/05', 'WYX/8408/5545'], ['20/05', 'paciente', 'bon estat general', 'bon descans nocturn', 'Centro COVID', 'suegros', '27/04/2011', '29/04/2011', 'Hospital', '3/05/2011', 'Azitromicina', 'buen estado general', 'hoy', 'paciente', 'paciente', 'WYX/8408/5545', 'paciente']]\n"
     ]
    }
   ],
   "source": [
    "anonimized(llm=SmallLlamaModel(), name_model=\"small_llama3\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'data/anon/raw/big_mistral' created successfully\n",
      "[['hilandero', 'suegro', '2 yernos', '8/9/21', '7/9/21', '976028134', '983425634', '8/9/21', '7/9/21', '8/9/21', '7/9/21', '8/9/21', '7/9/21', 'Salud laboral', '15/09', '10.09.2021'], ['hilandero', 'suegro', '2 yernos', '8/9/21', '7/9/21', '976028134', '983425634', '8/9/21', '7/9/21', '8/9/21', '7/9/21', '15/09', '8/9/21', '7/9/21', '10.09.2021']]\n",
      "[['20/05', 'Centro COVID', 'suegros', '27/04/2011', '29/04/2011', 'Hospital COVID', '3/05/2011', '30/04', '7/05', '7/05', '18/05', 'Centro COVID', 'suegros', '27/04/2011', '29/04/2011', '3/05/2011', '30/04', '7/05', '7/05', 'WYX/8408/5545'], ['36,4', 'suegros', '27/04/2011', '29/04/2011', '3/05/2011', '0.40', '177', '200', '2100', 'suegros', '27/04/2011', '29/04/2011', '3/05/2011', '0.40', '177', '200', '2100', '35,8', 'WYX/8408/5545']]\n"
     ]
    }
   ],
   "source": [
    "anonimized(llm=BigMistralModel(), name_model=\"big_mistral\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
